{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "The following dataset is with repsect to NBA Shots. Here we're ensuring that the data is cleaned up which allows us to run analysis on it easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SEASON_1 SEASON_2     TEAM_ID               TEAM_NAME  PLAYER_ID  \\\n",
      "0      2004  2003-04  1610612747      Los Angeles Lakers        977   \n",
      "1      2004  2003-04  1610612757  Portland Trail Blazers        757   \n",
      "2      2004  2003-04  1610612747      Los Angeles Lakers        977   \n",
      "3      2004  2003-04  1610612757  Portland Trail Blazers        757   \n",
      "4      2004  2003-04  1610612757  Portland Trail Blazers        757   \n",
      "\n",
      "        PLAYER_NAME POSITION_GROUP POSITION   GAME_DATE   GAME_ID  ...  \\\n",
      "0       Kobe Bryant              G       SG  04-14-2004  20301187  ...   \n",
      "1  Damon Stoudamire              G       PG  04-14-2004  20301187  ...   \n",
      "2       Kobe Bryant              G       SG  04-14-2004  20301187  ...   \n",
      "3  Damon Stoudamire              G       PG  04-14-2004  20301187  ...   \n",
      "4  Damon Stoudamire              G       PG  04-14-2004  20301187  ...   \n",
      "\n",
      "          BASIC_ZONE         ZONE_NAME ZONE_ABB       ZONE_RANGE LOC_X  LOC_Y  \\\n",
      "0  Above the Break 3  Left Side Center       LC          24+ ft.  20.0  21.35   \n",
      "1    Restricted Area            Center        C  Less Than 8 ft.  -0.0   5.25   \n",
      "2          Mid-Range  Left Side Center       LC        16-24 ft.  13.3  24.45   \n",
      "3          Mid-Range         Left Side        L        16-24 ft.  16.4  13.95   \n",
      "4          Mid-Range        Right Side        R        16-24 ft. -15.8   7.85   \n",
      "\n",
      "  SHOT_DISTANCE QUARTER MINS_LEFT SECS_LEFT  \n",
      "0            25       6         0         0  \n",
      "1             0       6         0         2  \n",
      "2            23       6         0         9  \n",
      "3            18       6         0        31  \n",
      "4            16       6         0        55  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('NBA_2004_Shots.csv')\n",
    "\n",
    "# Display few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing unecessary data points\n",
    "Here we intend to remove the following datasets with the following reasons:\n",
    "\n",
    "1. Season_2, Team_ID, Team_Name: These provide context but don't directly influence the decision to take a shot\n",
    "2. Game_Date, Game_ID: The specific games and dates are less relevant unless analyzing trends or specific matchups\n",
    "3. Home_Team, Away_Team: No way to match team information with abbreviated team data types\n",
    "4. Quarter, mins_left, secs_left: can be condensed into time passed\n",
    "\n",
    "# Combining columns and condensing them into relevant information\n",
    "1. Condensing quarter, mins left and secs left into time passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SEASON_1 SEASON_2     TEAM_ID               TEAM_NAME  PLAYER_ID  \\\n",
      "0      2004  2003-04  1610612747      Los Angeles Lakers        977   \n",
      "1      2004  2003-04  1610612757  Portland Trail Blazers        757   \n",
      "2      2004  2003-04  1610612747      Los Angeles Lakers        977   \n",
      "3      2004  2003-04  1610612757  Portland Trail Blazers        757   \n",
      "4      2004  2003-04  1610612757  Portland Trail Blazers        757   \n",
      "\n",
      "        PLAYER_NAME POSITION_GROUP POSITION   GAME_DATE   GAME_ID  ...  \\\n",
      "0       Kobe Bryant              G       SG  04-14-2004  20301187  ...   \n",
      "1  Damon Stoudamire              G       PG  04-14-2004  20301187  ...   \n",
      "2       Kobe Bryant              G       SG  04-14-2004  20301187  ...   \n",
      "3  Damon Stoudamire              G       PG  04-14-2004  20301187  ...   \n",
      "4  Damon Stoudamire              G       PG  04-14-2004  20301187  ...   \n",
      "\n",
      "          ZONE_NAME ZONE_ABB       ZONE_RANGE  LOC_X  LOC_Y SHOT_DISTANCE  \\\n",
      "0  Left Side Center       LC          24+ ft.   20.0  21.35            25   \n",
      "1            Center        C  Less Than 8 ft.   -0.0   5.25             0   \n",
      "2  Left Side Center       LC        16-24 ft.   13.3  24.45            23   \n",
      "3         Left Side        L        16-24 ft.   16.4  13.95            18   \n",
      "4        Right Side        R        16-24 ft.  -15.8   7.85            16   \n",
      "\n",
      "  QUARTER MINS_LEFT SECS_LEFT TIME_PASSED  \n",
      "0       6         0         0        3000  \n",
      "1       6         0         2        3002  \n",
      "2       6         0         9        3009  \n",
      "3       6         0        31        3031  \n",
      "4       6         0        55        3055  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "Index(['SEASON_1', 'PLAYER_ID', 'PLAYER_NAME', 'POSITION_GROUP', 'POSITION',\n",
      "       'SHOT_MADE', 'ACTION_TYPE', 'SHOT_TYPE', 'BASIC_ZONE', 'ZONE_NAME',\n",
      "       'ZONE_ABB', 'ZONE_RANGE', 'LOC_X', 'LOC_Y', 'SHOT_DISTANCE',\n",
      "       'TIME_PASSED'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# convert the quarters, mins and secs columns into a new column called time_passed in seconds ((quarter-1)*10*60 + mins*60 + secs)\n",
    "df['TIME_PASSED'] = (df['QUARTER']-1)*10*60 + df['MINS_LEFT']*60 + df['SECS_LEFT']\n",
    "\n",
    "# print the first few rows of the dataframe\n",
    "print(df.head())\n",
    "df = df.drop(['SEASON_2', 'TEAM_ID', 'TEAM_NAME', 'GAME_DATE', 'GAME_ID', 'HOME_TEAM', 'AWAY_TEAM', 'EVENT_TYPE', 'QUARTER', 'MINS_LEFT', 'SECS_LEFT'], axis=1)\n",
    "\n",
    "# print the remaining columns\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITION_GROUP: 3\n",
      "POSITION: 12\n",
      "ACTION_TYPE: 29\n",
      "SHOT_TYPE: 2\n",
      "BASIC_ZONE: 7\n",
      "ZONE_NAME: 6\n",
      "ZONE_ABB: 6\n",
      "ZONE_RANGE: 5\n"
     ]
    }
   ],
   "source": [
    "# Column visualisation \n",
    "print(\"POSITION_GROUP: \" + str(df['POSITION_GROUP'].nunique()))\n",
    "print(\"POSITION: \" + str(df['POSITION'].nunique()))\n",
    "print(\"ACTION_TYPE: \" + str(df['ACTION_TYPE'].nunique()))\n",
    "print(\"SHOT_TYPE: \" + str(df['SHOT_TYPE'].nunique()))\n",
    "print(\"BASIC_ZONE: \" + str(df['BASIC_ZONE'].nunique()))\n",
    "print(\"ZONE_NAME: \" + str(df['ZONE_NAME'].nunique()))\n",
    "print(\"ZONE_ABB: \" + str(df['ZONE_ABB'].nunique()))\n",
    "print(\"ZONE_RANGE: \" + str(df['ZONE_RANGE'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SEASON_1', 'PLAYER_ID', 'PLAYER_NAME', 'POSITION_GROUP', 'POSITION',\n",
      "       'SHOT_MADE', 'ACTION_TYPE', 'SHOT_TYPE', 'BASIC_ZONE', 'ZONE_NAME',\n",
      "       'ZONE_ABB', 'ZONE_RANGE', 'LOC_X', 'LOC_Y', 'SHOT_DISTANCE',\n",
      "       'TIME_PASSED'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print columns\n",
    "print(df.columns)\n",
    "\n",
    "training_df = df.drop(['SEASON_1', 'PLAYER_NAME','ZONE_NAME', 'LOC_X','LOC_Y'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITION_GROUP: ['G' 'C' 'F']\n",
      "POSITION: ['SG' 'PG' 'C' 'PF' 'SF' 'SF-SG' 'C-PF' 'SG-SF' 'PG-SF' 'SG-PG' 'PF-C'\n",
      " 'PF-SF']\n",
      "SHOT_MADE: [ True False]\n",
      "ACTION_TYPE: ['Jump Shot' 'Driving Layup Shot' 'Dunk Shot' 'Alley Oop Dunk Shot'\n",
      " 'Fadeaway Jump Shot' 'Driving Finger Roll Shot' 'Layup Shot' 'Tip Shot'\n",
      " 'Slam Dunk Shot' 'Running Hook Shot' 'Turnaround Jump Shot'\n",
      " 'Running Layup Shot' 'Jump Bank Shot' 'Running Jump Shot'\n",
      " 'Jump Hook Shot' 'Reverse Layup Shot' 'Driving Dunk Shot' 'Hook Shot'\n",
      " 'Reverse Dunk Shot' 'Running Finger Roll Shot' 'Turnaround Hook Shot'\n",
      " 'Alley Oop Layup shot' 'Follow Up Dunk Shot' 'Driving Hook Shot'\n",
      " 'Running Dunk Shot' 'Finger Roll Shot' 'Hook Bank Shot'\n",
      " 'Turnaround Finger Roll Shot' 'Running Tip Shot']\n",
      "SHOT_TYPE: ['3PT Field Goal' '2PT Field Goal']\n",
      "BASIC_ZONE: ['Above the Break 3' 'Restricted Area' 'Mid-Range' 'Left Corner 3'\n",
      " 'In The Paint (Non-RA)' 'Right Corner 3' 'Backcourt']\n",
      "ZONE_ABB: ['LC' 'C' 'L' 'R' 'RC' 'BC']\n",
      "ZONE_RANGE: ['24+ ft.' 'Less Than 8 ft.' '16-24 ft.' '8-16 ft.' 'Back Court Shot']\n",
      "SHOT_DISTANCE: [25  0 23 18 16 24  1  9  3 17  2 20 28 26 14 22  4 11  5  8 10 35 21 15\n",
      " 27 12 19  7  6 13 41 42 40 43 33 46 45 75 30 32 59 39 68 54 72 74 71 48\n",
      " 44 31 57 29 56 47 37 51 69 77 34 58 38 62 81 36 50 79 49 53 78 61 67 55\n",
      " 52 65 64 60 76 83 80 70 63 87 73 66]\n"
     ]
    }
   ],
   "source": [
    "for column in training_df.columns:\n",
    "    if column == 'PLAYER_ID' or column == 'TIME_PASSED':\n",
    "        continue\n",
    "    else:\n",
    "        print(column + \": \" + str(training_df[column].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189803, 10)\n"
     ]
    }
   ],
   "source": [
    "# Drop ZONE_ABB\n",
    "training_df = training_df.drop(['ZONE_ABB'], axis = 1)\n",
    "\n",
    "# size of df\n",
    "print(training_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Process data for KMEans and DBSCAN \n",
    "These two methods are dependent on distance metrics so we need to ensure that the following categorical variables (POSITION_TYPE, ACTION_TYPE, BASIC_ZONE, ZONE_RANGE) are encoded using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans, DBSCAN\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import time\n",
    "\n",
    "\n",
    "le_position_group = LabelEncoder()\n",
    "training_df['POSITION_GROUP'] = le_position_group.fit_transform(training_df['POSITION_GROUP'])\n",
    "\n",
    "le_position = LabelEncoder()\n",
    "training_df['POSITION'] = le_position.fit_transform(training_df['POSITION'])\n",
    "\n",
    "le_action_type = LabelEncoder()\n",
    "training_df['ACTION_TYPE'] = le_action_type.fit_transform(training_df['ACTION_TYPE'])\n",
    "\n",
    "le_shot_type = LabelEncoder()\n",
    "training_df['SHOT_TYPE'] = le_shot_type.fit_transform(training_df['SHOT_TYPE'])\n",
    "\n",
    "# Define features for clustering (excluding SHOT_MADE as it's a target feature)\n",
    "features = ['TIME_PASSED', 'POSITION_GROUP', 'POSITION', 'ACTION_TYPE', 'SHOT_TYPE', 'SHOT_DISTANCE']\n",
    "X = training_df[features]\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajeshkumar/SC4020_NBA_Dataset/nba_proj/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniBatch KMeans finished in 0.9905319213867188 seconds.\n",
      "DBSCAN finished in 19.687666177749634 seconds.\n",
      "Bayesian Gaussian Mixture finished in 3.7150752544403076 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Apply MiniBatch KMeans for large datasets\n",
    "start_time = time.time()\n",
    "kmeans = MiniBatchKMeans(n_clusters=5, batch_size=10000, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "print(f\"MiniBatch KMeans finished in {time.time() - start_time} seconds.\")\n",
    "\n",
    "# Step 3: Apply DBSCAN clustering (might require tuning of eps and min_samples)\n",
    "start_time = time.time()\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5, n_jobs=-1)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "print(f\"DBSCAN finished in {time.time() - start_time} seconds.\")\n",
    "\n",
    "# Step 4: Apply Bayesian Gaussian Mixture clustering\n",
    "start_time = time.time()\n",
    "bayes = BayesianGaussianMixture(n_components=5, random_state=42)\n",
    "bayes_labels = bayes.fit_predict(X_scaled)\n",
    "print(f\"Bayesian Gaussian Mixture finished in {time.time() - start_time} seconds.\")\n",
    "\n",
    "# Add cluster labels to the original DataFrame for analysis\n",
    "training_df['KMeans_Cluster'] = kmeans_labels\n",
    "training_df['DBSCAN_Cluster'] = dbscan_labels\n",
    "training_df['Bayes_Cluster'] = bayes_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans Cluster - SHOT_MADE Success Rate:\n",
      "KMeans_Cluster\n",
      "0    0.415255\n",
      "1    0.375710\n",
      "2    0.347139\n",
      "3    0.456486\n",
      "4    0.767366\n",
      "Name: SHOT_MADE, dtype: float64\n",
      "\n",
      "DBSCAN Cluster - SHOT_MADE Success Rate:\n",
      "DBSCAN_Cluster\n",
      "-1     0.230216\n",
      " 0     0.350396\n",
      " 1     0.441557\n",
      " 2     0.344272\n",
      " 3     0.471094\n",
      " 4     0.361386\n",
      " 5     0.443597\n",
      " 6     0.457964\n",
      " 7     0.483202\n",
      " 8     0.336991\n",
      " 9     0.284895\n",
      " 10    0.000000\n",
      " 11    0.000000\n",
      " 12    0.200000\n",
      " 13    0.000000\n",
      " 14    0.222222\n",
      " 15    0.000000\n",
      " 16    0.500000\n",
      " 17    0.000000\n",
      " 18    0.500000\n",
      " 19    0.000000\n",
      " 20    0.000000\n",
      "Name: SHOT_MADE, dtype: float64\n",
      "\n",
      "Bayes Cluster - SHOT_MADE Success Rate:\n",
      "Bayes_Cluster\n",
      "0    0.362297\n",
      "1    0.404913\n",
      "2    0.620852\n",
      "3    0.347167\n",
      "4    0.748569\n",
      "Name: SHOT_MADE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Post-clustering analysis - How representative are the clusters of SHOT_MADE?\n",
    "\n",
    "# KMeans Clustering: Calculate the mean success rate (SHOT_MADE) per cluster\n",
    "kmeans_analysis = training_df.groupby('KMeans_Cluster')['SHOT_MADE'].mean()\n",
    "print(\"KMeans Cluster - SHOT_MADE Success Rate:\")\n",
    "print(kmeans_analysis)\n",
    "\n",
    "# DBSCAN Clustering: Calculate the mean success rate (SHOT_MADE) per cluster\n",
    "dbscan_analysis = training_df.groupby('DBSCAN_Cluster')['SHOT_MADE'].mean()\n",
    "print(\"\\nDBSCAN Cluster - SHOT_MADE Success Rate:\")\n",
    "print(dbscan_analysis)\n",
    "\n",
    "# Bayesian Clustering: Calculate the mean success rate (SHOT_MADE) per cluster\n",
    "bayes_analysis = training_df.groupby('Bayes_Cluster')['SHOT_MADE'].mean()\n",
    "print(\"\\nBayes Cluster - SHOT_MADE Success Rate:\")\n",
    "print(bayes_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MiniBatch KMeans Silhouette Score: 0.2379887269860108\n",
      "MiniBatch KMeans DBI: 1.5091721553431416\n",
      "DBSCAN Silhouette Score: 0.2013338570254\n",
      "DBSCAN DBI: 1.8117748787177064\n",
      "Bayesian Silhouette Score: 0.30583279326929125\n",
      "Bayesian DBI: 1.3046953265286296\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate clustering quality using Silhouette Score and Davies-Bouldin Index\n",
    "\n",
    "# For KMeans\n",
    "silhouette_kmeans = silhouette_score(X_scaled, kmeans_labels)\n",
    "dbi_kmeans = davies_bouldin_score(X_scaled, kmeans_labels)\n",
    "print(f\"\\nMiniBatch KMeans Silhouette Score: {silhouette_kmeans}\")\n",
    "print(f\"MiniBatch KMeans DBI: {dbi_kmeans}\")\n",
    "\n",
    "# For DBSCAN\n",
    "if len(np.unique(dbscan_labels)) > 1:\n",
    "    silhouette_dbscan = silhouette_score(X_scaled, dbscan_labels)\n",
    "    dbi_dbscan = davies_bouldin_score(X_scaled, dbscan_labels)\n",
    "    print(f\"DBSCAN Silhouette Score: {silhouette_dbscan}\")\n",
    "    print(f\"DBSCAN DBI: {dbi_dbscan}\")\n",
    "else:\n",
    "    print(\"DBSCAN did not find more than one cluster\")\n",
    "\n",
    "# For Bayesian Gaussian Mixture\n",
    "silhouette_bayes = silhouette_score(X_scaled, bayes_labels)\n",
    "dbi_bayes = davies_bouldin_score(X_scaled, bayes_labels)\n",
    "print(f\"Bayesian Silhouette Score: {silhouette_bayes}\")\n",
    "print(f\"Bayesian DBI: {dbi_bayes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can we optimise the code above?\n",
    "\n",
    "1. Batch Sizing for Mini KMeans Clustering\n",
    "The size can potentially be reduced further if needed, current size is 10,000\n",
    "\n",
    "2. DBSCAN Parameters\n",
    "eps = 0.5 which might be a little too high for a dataset of our density. It might make sense to perform Principle Component Analaysis (PCA), a dimension reduction method, so that the dataset is easier to handle\n",
    "\n",
    "3. Handling Cluster Labels for DBSCAN\n",
    "Excluding noise where dbscan label is -1 would be important "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajeshkumar/SC4020_NBA_Dataset/nba_proj/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniBatch KMeans finished in 0.04029488563537598 seconds.\n",
      "DBSCAN finished in 231.86426496505737 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajeshkumar/SC4020_NBA_Dataset/nba_proj/lib/python3.8/site-packages/sklearn/mixture/_base.py:268: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Gaussian Mixture finished in 10.899379014968872 seconds.\n",
      "KMeans Cluster - SHOT_MADE Success Rate:\n",
      "KMeans_Cluster\n",
      "0    0.376841\n",
      "1    0.477883\n",
      "2    0.366108\n",
      "3    0.523372\n",
      "4    0.515614\n",
      "Name: SHOT_MADE, dtype: float64\n",
      "\n",
      "DBSCAN Cluster - SHOT_MADE Success Rate:\n",
      "DBSCAN_Cluster\n",
      "-1    0.230769\n",
      " 0    0.438659\n",
      " 1    0.285714\n",
      "Name: SHOT_MADE, dtype: float64\n",
      "\n",
      "Bayes Cluster - SHOT_MADE Success Rate:\n",
      "Bayes_Cluster\n",
      "0    0.369043\n",
      "1    0.596970\n",
      "2    0.405498\n",
      "3    0.381789\n",
      "4    0.422607\n",
      "Name: SHOT_MADE, dtype: float64\n",
      "\n",
      "MiniBatch KMeans Silhouette Score: 0.42039755669877105\n",
      "MiniBatch KMeans DBI: 0.835032030353854\n",
      "DBSCAN Silhouette Score: 0.505051536020271\n",
      "DBSCAN DBI: 0.7846465535065041\n",
      "Bayesian Silhouette Score: 0.1754291348013303\n",
      "Bayesian DBI: 1.5662926792312546\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans, DBSCAN\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import time\n",
    "\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "X = training_df[['SHOT_DISTANCE', 'TIME_PASSED']] \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% variance\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 3: Apply MiniBatch KMeans for large datasets\n",
    "start_time = time.time()\n",
    "kmeans = MiniBatchKMeans(n_clusters=5, batch_size=1000, random_state=42)  # Adjusted batch size\n",
    "kmeans_labels = kmeans.fit_predict(X_reduced)\n",
    "print(f\"MiniBatch KMeans finished in {time.time() - start_time} seconds.\")\n",
    "\n",
    "# Step 4: Apply DBSCAN clustering\n",
    "start_time = time.time()\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5, n_jobs=-1)  \n",
    "dbscan_labels = dbscan.fit_predict(X_reduced)\n",
    "print(f\"DBSCAN finished in {time.time() - start_time} seconds.\")\n",
    "\n",
    "# Step 5: Apply Bayesian Gaussian Mixture clustering\n",
    "start_time = time.time()\n",
    "bayes = BayesianGaussianMixture(n_components=5, random_state=42)\n",
    "bayes_labels = bayes.fit_predict(X_reduced)\n",
    "print(f\"Bayesian Gaussian Mixture finished in {time.time() - start_time} seconds.\")\n",
    "\n",
    "# Add cluster labels to the original DataFrame for analysis\n",
    "training_df['KMeans_Cluster'] = kmeans_labels\n",
    "training_df['DBSCAN_Cluster'] = dbscan_labels\n",
    "training_df['Bayes_Cluster'] = bayes_labels\n",
    "\n",
    "# KMeans Clustering\n",
    "kmeans_analysis = training_df.groupby('KMeans_Cluster')['SHOT_MADE'].mean()\n",
    "print(\"KMeans Cluster - SHOT_MADE Success Rate:\")\n",
    "print(kmeans_analysis)\n",
    "\n",
    "# DBSCAN Clustering\n",
    "dbscan_analysis = training_df.groupby('DBSCAN_Cluster')['SHOT_MADE'].mean()\n",
    "print(\"\\nDBSCAN Cluster - SHOT_MADE Success Rate:\")\n",
    "print(dbscan_analysis)\n",
    "\n",
    "# Bayesian Clustering\n",
    "bayes_analysis = training_df.groupby('Bayes_Cluster')['SHOT_MADE'].mean()\n",
    "print(\"\\nBayes Cluster - SHOT_MADE Success Rate:\")\n",
    "print(bayes_analysis)\n",
    "\n",
    "# Step 7: Evaluate clustering quality using Silhouette Score and Davies-Bouldin Index\n",
    "# For KMeans\n",
    "silhouette_kmeans = silhouette_score(X_reduced, kmeans_labels)\n",
    "dbi_kmeans = davies_bouldin_score(X_reduced, kmeans_labels)\n",
    "print(f\"\\nMiniBatch KMeans Silhouette Score: {silhouette_kmeans}\")\n",
    "print(f\"MiniBatch KMeans DBI: {dbi_kmeans}\")\n",
    "\n",
    "# For DBSCAN\n",
    "if len(np.unique(dbscan_labels)) > 1:\n",
    "    silhouette_dbscan = silhouette_score(X_reduced, dbscan_labels)\n",
    "    dbi_dbscan = davies_bouldin_score(X_reduced, dbscan_labels)\n",
    "    print(f\"DBSCAN Silhouette Score: {silhouette_dbscan}\")\n",
    "    print(f\"DBSCAN DBI: {dbi_dbscan}\")\n",
    "else:\n",
    "    print(\"DBSCAN did not find more than one cluster, so Silhouette and DBI scores are not applicable.\")\n",
    "\n",
    "# For Bayesian Gaussian Mixture\n",
    "silhouette_bayes = silhouette_score(X_reduced, bayes_labels)\n",
    "dbi_bayes = davies_bouldin_score(X_reduced, bayes_labels)\n",
    "print(f\"Bayesian Silhouette Score: {silhouette_bayes}\")\n",
    "print(f\"Bayesian DBI: {dbi_bayes}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
